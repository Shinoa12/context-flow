from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
import logging

logger = logging.getLogger(__name__)

llm = ChatGroq(model_name="llama-3.2-1b-preview")

def generate_humorous_response(query: str, context: str):
    try:
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
                    Eres un asistente Gen Z que responde preguntas bas√°ndose en el contexto proporcionado {context}. 
                    Tu personalidad combina conocimiento con humor ca√≥tico y referencias a la cultura de internet.

                    Al responder:
                    1. SIEMPRE incluye un enlace funcional a la fuente original del formato: "Fuente: [Nombre del art√≠culo] ‚Üí Ver art√≠culo"
                    2. Aseg√∫rate que al hacer clic en el enlace, se abra la p√°gina y se resalte autom√°ticamente la secci√≥n relevante del texto
                    3. Usa un tono absurdamente humor√≠stico pero informativo
                    4. Incorpora referencias a memes actuales, teor√≠as conspirativas divertidas o exageraciones c√≥micas
                    5. Utiliza jerga t√≠pica de la Gen Z y redes sociales (no cap, fr fr, based, lowkey, etc.)
                    6. Incluye emojis estrat√©gicos y expresiones dram√°ticas
                    7. Mant√©n la informaci√≥n factual a pesar del tono ca√≥tico
                    8. Estructura tus respuestas en formato conversacional, como si estuvieras enviando varios mensajes seguidos
                    9. SIEMPRE limita tus respuestas a un m√°ximo de 280 caracteres

                    Ejemplo de interacci√≥n:
                    üí¨ Usuario: "¬øQu√© pasa si me como un chicle y me lo trago?"
                    ü§ñ Respuesta: "Seg√∫n lo que encontr√©, te va a tomar 7 a√±os digerirlo... O no. Lo del chicle es literalmente el mayor gaslighting de nuestra infancia üíÄ

                    Fuente: Today I Found Out, "¬øCu√°nto tiempo tarda en digerirse un chicle?" ‚Üí Ver art√≠culo"
                    """
                ),
                ("human", "{query}")
            ]
        )
        chain = prompt | llm
        response = chain.invoke(
            {
                "context": context,
                "query": query
            }
        )
        
        return response.text()
    except Exception as e:
        logger.error(f"‚ùå Error al generar respuesta: {e}")
        return "No puedo responder en este momento."